{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing=fetch_california_housing()\n",
    "\n",
    "X_train_full,X_test,y_train_full,y_test=train_test_split(housing.data,housing.target)\n",
    "X_train,X_valid,y_train,y_valid=train_test_split(X_train_full,y_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11610, 8)\n",
      "(3870, 8)\n",
      "(5160, 8)\n"
     ]
    }
   ],
   "source": [
    "scaler=StandardScaler() \n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_valid=scaler.transform(X_valid)\n",
    "X_test=scaler.transform(X_test)\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StandardScalerは標準化で，MinMaxScalerは正規化のモジュール\n",
    "### fit : パラメータ（平均や標準偏差など）を計算\n",
    "### transform : パラメータをもとにデータ変換\n",
    "### fit_transform : パラメータ計算とデータ変換をまとめて実行\n",
    "- 今回は，最初にfit_transformをすることでパラメータ計算とデータ変換を実行．計算したパラメータをscalerが保持してるので，その後に続く処理ではデータ変換だけでよい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 8)\n",
      "(None, 30)\n",
      "(None, 30)\n",
      "(None, 38)\n"
     ]
    }
   ],
   "source": [
    "input_ =keras.layers.Input(shape=X_train.shape[1:])#X_train.shape[1:]=8\n",
    "hidden1=keras.layers.Dense(30,activation=\"relu\")(input_)\n",
    "hidden2=keras.layers.Dense(30,activation=\"relu\")(hidden1)\n",
    "concat=keras.layers.Concatenate()([input_,hidden2])\n",
    "output=keras.layers.Dense(1)(concat)\n",
    "model=keras.Model(inputs=[input_],outputs=[output])\n",
    "print(input_.shape)\n",
    "print(hidden1.shape)\n",
    "print(hidden2.shape)\n",
    "print(concat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 入力が２つで，出力が1つの場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A=keras.layers.Input(shape=[5],name='wide_input')\n",
    "input_B=keras.layers.Input(shape=[6],name=\"deep_input\")\n",
    "hidden1=keras.layers.Dense(30,activation=\"relu\")(input_B)\n",
    "hidden2=keras.layers.Dense(30,activation=\"relu\")(hidden1)\n",
    "concat=keras.layers.concatenate([input_A,hidden2])\n",
    "output=keras.layers.Dense(1,name=\"output\")(concat)\n",
    "model=keras.Model(inputs=[input_A,input_B],outputs=[output])\n",
    "model.compile(loss=\"mse\",optimizer=keras.optimizers.SGD(lr=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11610, 5)\n"
     ]
    }
   ],
   "source": [
    "X_train_A,X_train_B=X_train[:,:5],X_train[:,2:]\n",
    "X_valid_A,X_valid_B=X_valid[:,:5],X_valid[:,2:]\n",
    "X_test_A,X_test_B  =X_test[:,:5],X_test[:,2:]\n",
    "X_new_A,X_new_B=X_test_A[:3],X_test_B[:3]\n",
    "print(X_train_A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 0s 38us/sample - loss: 0.4769 - val_loss: 0.4930\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 0s 37us/sample - loss: 0.4742 - val_loss: 0.4927\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 0s 38us/sample - loss: 0.4727 - val_loss: 0.4879\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 0s 36us/sample - loss: 0.4707 - val_loss: 0.4881\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 0s 36us/sample - loss: 0.4689 - val_loss: 0.4850\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 0s 36us/sample - loss: 0.4678 - val_loss: 0.4830\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 0s 38us/sample - loss: 0.4660 - val_loss: 0.4828\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 0s 36us/sample - loss: 0.4648 - val_loss: 0.4798\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 0s 36us/sample - loss: 0.4630 - val_loss: 0.4774\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 0s 37us/sample - loss: 0.4617 - val_loss: 0.4762\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 0s 36us/sample - loss: 0.4604 - val_loss: 0.4756\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 0s 37us/sample - loss: 0.4586 - val_loss: 0.4734\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 0s 37us/sample - loss: 0.4579 - val_loss: 0.4738\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 0s 36us/sample - loss: 0.4565 - val_loss: 0.4721\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 0s 36us/sample - loss: 0.4554 - val_loss: 0.4698\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 0s 36us/sample - loss: 0.4543 - val_loss: 0.4688\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 0s 36us/sample - loss: 0.4530 - val_loss: 0.4699\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 0s 36us/sample - loss: 0.4515 - val_loss: 0.4708\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 0s 36us/sample - loss: 0.4507 - val_loss: 0.4648\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 0s 37us/sample - loss: 0.4494 - val_loss: 0.4644\n",
      "5160/5160 [==============================] - 0s 20us/sample - loss: 0.4341\n"
     ]
    }
   ],
   "source": [
    "history=model.fit((X_train_A,X_train_B),y_train,epochs=20,validation_data=((X_valid_A,X_valid_B),y_valid))\n",
    "mse_test=model.evaluate((X_test_A,X_test_B),y_test)\n",
    "y_pred=model.predict((X_new_A,X_new_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 入力２つ，出力も2つの場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=keras.layers.Dense(1,name=\"main_output\")(concat)\n",
    "aux_output=keras.layers.Dense(1,name=\"aux_output\")(hidden2)\n",
    "model=keras.Model(inputs=[input_A,input_B],outputs=[output,aux_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 1s 76us/sample - loss: 0.7797 - main_output_loss: 0.7130 - aux_output_loss: 1.3810 - val_loss: 1.9584 - val_main_output_loss: 2.0411 - val_aux_output_loss: 1.2064\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.5815 - main_output_loss: 0.5513 - aux_output_loss: 0.8527 - val_loss: 0.4904 - val_main_output_loss: 0.4571 - val_aux_output_loss: 0.7910\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.4823 - main_output_loss: 0.4526 - aux_output_loss: 0.7495 - val_loss: 0.4590 - val_main_output_loss: 0.4302 - val_aux_output_loss: 0.7184\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.4638 - main_output_loss: 0.4383 - aux_output_loss: 0.6939 - val_loss: 0.4476 - val_main_output_loss: 0.4217 - val_aux_output_loss: 0.6816\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.4519 - main_output_loss: 0.4288 - aux_output_loss: 0.6595 - val_loss: 0.4394 - val_main_output_loss: 0.4156 - val_aux_output_loss: 0.6548\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.4434 - main_output_loss: 0.4214 - aux_output_loss: 0.6407 - val_loss: 0.4507 - val_main_output_loss: 0.4286 - val_aux_output_loss: 0.6500\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.4302 - main_output_loss: 0.4090 - aux_output_loss: 0.6206 - val_loss: 0.4236 - val_main_output_loss: 0.4017 - val_aux_output_loss: 0.6216\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 0s 38us/sample - loss: 0.4226 - main_output_loss: 0.4021 - aux_output_loss: 0.6070 - val_loss: 0.4174 - val_main_output_loss: 0.3963 - val_aux_output_loss: 0.6083\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 0s 43us/sample - loss: 0.4187 - main_output_loss: 0.3986 - aux_output_loss: 0.5989 - val_loss: 0.4048 - val_main_output_loss: 0.3849 - val_aux_output_loss: 0.5846\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 0s 41us/sample - loss: 0.4432 - main_output_loss: 0.4271 - aux_output_loss: 0.5871 - val_loss: 0.4103 - val_main_output_loss: 0.3906 - val_aux_output_loss: 0.5885\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 0s 41us/sample - loss: 0.4113 - main_output_loss: 0.3927 - aux_output_loss: 0.5779 - val_loss: 0.4021 - val_main_output_loss: 0.3834 - val_aux_output_loss: 0.5715\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 0s 37us/sample - loss: 0.4040 - main_output_loss: 0.3861 - aux_output_loss: 0.5645 - val_loss: 0.3968 - val_main_output_loss: 0.3781 - val_aux_output_loss: 0.5657\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.3989 - main_output_loss: 0.3814 - aux_output_loss: 0.5568 - val_loss: 0.3991 - val_main_output_loss: 0.3804 - val_aux_output_loss: 0.5671\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.3897 - main_output_loss: 0.3719 - aux_output_loss: 0.5496 - val_loss: 0.4005 - val_main_output_loss: 0.3838 - val_aux_output_loss: 0.5513\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 0s 38us/sample - loss: 0.4022 - main_output_loss: 0.3862 - aux_output_loss: 0.5456 - val_loss: 0.3862 - val_main_output_loss: 0.3697 - val_aux_output_loss: 0.5351\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.3811 - main_output_loss: 0.3643 - aux_output_loss: 0.5315 - val_loss: 0.3896 - val_main_output_loss: 0.3733 - val_aux_output_loss: 0.5368\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 0s 38us/sample - loss: 0.3806 - main_output_loss: 0.3650 - aux_output_loss: 0.5218 - val_loss: 0.3806 - val_main_output_loss: 0.3657 - val_aux_output_loss: 0.5157\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 0s 40us/sample - loss: 0.3724 - main_output_loss: 0.3568 - aux_output_loss: 0.5123 - val_loss: 0.3788 - val_main_output_loss: 0.3636 - val_aux_output_loss: 0.5163\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3707 - main_output_loss: 0.3552 - aux_output_loss: 0.5096 - val_loss: 0.3702 - val_main_output_loss: 0.3563 - val_aux_output_loss: 0.4961\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 0s 41us/sample - loss: 0.3662 - main_output_loss: 0.3512 - aux_output_loss: 0.4998 - val_loss: 0.3766 - val_main_output_loss: 0.3623 - val_aux_output_loss: 0.5058\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=[\"mse\",\"mse\"],loss_weights=[0.9,0.1],optimizer=\"sgd\")\n",
    "history=model.fit([X_train_A,X_train_B],[y_train,y_train],epochs=20,validation_data=([X_valid_A,X_valid_B],[y_valid,y_valid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5160/5160 [==============================] - 0s 22us/sample - loss: 0.3711 - main_output_loss: 0.3556 - aux_output_loss: 0.5019\n"
     ]
    }
   ],
   "source": [
    "total_loss,main_loss,aux_loss=model.evaluate([X_test_A,X_test_B],[y_test,y_test])\n",
    "y_pred_main,y_pred_aux=model.predict([X_new_A,X_new_B])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Subclassing API to Build Dynamic Models\n",
    "### Functional APIを試してみよう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(keras.Model):\n",
    "    def __init__(self,units=30,activation=\"relu\",**kwargs):\n",
    "        super().__init__(**kwargs) #handles standard args (e.g., name)\n",
    "        self.hidden1=keras.layers.Dense(units,activation=activation)\n",
    "        self.hidden2=keras.layers.Dense(units,activation=activation)\n",
    "        self.main_output=keras.layers.Dense(1)\n",
    "        self.aux_output=keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self,inputs):\n",
    "        input_A,input_B=inputs\n",
    "        hidden1=self.hidden1(input_B)\n",
    "        hidden2=self.hidden2(hidden1)\n",
    "        concat=keras.layers.concatenate([input_A,hidden2])\n",
    "        main_output=self.main_output(concat)\n",
    "        aux_output=self.aux_output(hidden2)\n",
    "        return main_output, aux_output\n",
    "    \n",
    "model=WideAndDeepModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 型のチェックなどが事前にできないので，Functional API は不便．極力Sequential APIを活用するべき"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelを保存しよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ModelをLoadしよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=keras.models.load_model(\"my_keras_model.h5\") #roll back to best model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- save_weights()やload_weights()を使うことでモデルパラメータを保存できるが，それ以外は全て自分で保存する必要がある"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbackメソッドを使いチェックポイントで保存する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples\n",
      "Epoch 1/10\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 0.4368\n",
      "Epoch 2/10\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.4358\n",
      "Epoch 3/10\n",
      "11610/11610 [==============================] - 0s 34us/sample - loss: 0.4349\n",
      "Epoch 4/10\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: 0.4335\n",
      "Epoch 5/10\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.4329\n",
      "Epoch 6/10\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: 0.4316\n",
      "Epoch 7/10\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.4307\n",
      "Epoch 8/10\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: 0.4295\n",
      "Epoch 9/10\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.4287\n",
      "Epoch 10/10\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 0.4278\n"
     ]
    }
   ],
   "source": [
    "checkpoint_cb=keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\")\n",
    "history=model.fit([X_train_A,X_train_B],y_train,epochs=10,callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), for inputs ['wide_input', 'deep_input'] but instead got the following list of 1 arrays: [array([[-0.84543428, -0.99829321,  0.12468852, ...,  0.01012836,\n         2.22308376, -0.51138938],\n       [-0.12143853, -0.04555135, -0.30930892, ..., -0.0060048 ,\n        -0.81508065,  0.84520221],...",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-f81b56b5c0ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mearly_stopping_cb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrestore_best_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhistory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheckpoint_cb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mearly_stopping_cb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\Keras_training\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Keras_training\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Keras_training\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    591\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    594\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Keras_training\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    644\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m     x, y, sample_weights = standardize(\n\u001b[1;32m--> 646\u001b[1;33m         x, y, sample_weight=sample_weights)\n\u001b[0m\u001b[0;32m    647\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0madapter_cls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mListsOfScalarsDataAdapter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Keras_training\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2381\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2382\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2383\u001b[1;33m         batch_size=batch_size)\n\u001b[0m\u001b[0;32m   2384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2385\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Keras_training\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[1;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   2408\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2409\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2410\u001b[1;33m           exception_prefix='input')\n\u001b[0m\u001b[0;32m   2411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2412\u001b[0m     \u001b[1;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Keras_training\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    537\u001b[0m                        \u001b[1;34m'for inputs '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but instead got the '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    538\u001b[0m                        \u001b[1;34m'following list of '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' arrays: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 539\u001b[1;33m                        str(data)[:200] + '...')\n\u001b[0m\u001b[0;32m    540\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m       raise ValueError('Error when checking model ' + exception_prefix +\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), for inputs ['wide_input', 'deep_input'] but instead got the following list of 1 arrays: [array([[-0.84543428, -0.99829321,  0.12468852, ...,  0.01012836,\n         2.22308376, -0.51138938],\n       [-0.12143853, -0.04555135, -0.30930892, ..., -0.0060048 ,\n        -0.81508065,  0.84520221],..."
     ]
    }
   ],
   "source": [
    "early_stopping_cb=keras.callbacks.EarlyStopping(patience=10,restore_best_weights=True)\n",
    "history=model.fit(X_train,y_train,epochs=100,validation_data=(X_valid,y_valid),callbacks=[checkpoint_cb,early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
